name: VM Integration Tests

# This workflow runs the FULL integration test suite on a self-hosted VM runner.
# It covers core datastore (directory/removable), metrics, jobs, notifications, and
# S3 integration tests when cloud credentials are available.
#
# Code coverage is collected during test runs and reported via Codecov.
# Coverage annotations will appear on pull requests showing which lines are covered.
# Coverage reports and trends are available at https://codecov.io

# - name: Trigger Workflow in Another Repository
    #   run: |
    #     # Set the required variables
    #     repo_owner=${{github.repository_owner}} 
    #     repo_name="lab2"  
    #     event_type="trigger-workflow" 
    #     service=${{ github.event.inputs.target_service }}"
    #     version="${{ github.event.inputs.target_version }}"

    #     curl -L \
    #       -X POST \
    #       -H "Accept: application/vnd.github+json" \
    #       -H "Authorization: Bearer ${{ secrets.GH_PAT }}" \
    #       -H "X-GitHub-Api-Version: 2022-11-28" \
    #       https://api.github.com/repos/$repo_owner/$repo_name/dispatches \
    #       -d "{\"event_type\": \"$event_type\", \"client_payload\": {\"service\": \"$service\", \"version\": \"$version\", \"unit\": false, \"integration\": true}}"

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  schedule:
    # Run weekly on Sunday at 2 AM UTC to test with full VM
    - cron: '0 2 * * 0'

jobs:
  vm-integration-tests:
    name: VM Integration Tests (PBS 4.0)
    runs-on: self-hosted
    timeout-minutes: 45
    
    # Service containers for testing - exposed on host network
    services:
      influxdb:
        image: influxdb:2.7-alpine
        ports:
          - 8086:8086
        env:
          DOCKER_INFLUXDB_INIT_MODE: setup
          DOCKER_INFLUXDB_INIT_USERNAME: admin
          DOCKER_INFLUXDB_INIT_PASSWORD: testpass123
          DOCKER_INFLUXDB_INIT_ORG: testorg
          DOCKER_INFLUXDB_INIT_BUCKET: pbs-metrics
          DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: test-token-for-pbs-provider
        options: >-
          --health-cmd "influx ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      influxdb-udp:
        image: influxdb:1.8
        ports:
          - 8089:8089/udp
        env:
          INFLUXDB_UDP_ENABLED: true
          INFLUXDB_UDP_BIND_ADDRESS: :8089
          INFLUXDB_UDP_DATABASE: udp
        options: >-
          --health-cmd "influx -execute 'SHOW DATABASES'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      gotify:
        image: gotify/server:latest
        ports:
          - 8080:80
        env:
          GOTIFY_DEFAULTUSER_NAME: admin
          GOTIFY_DEFAULTUSER_PASS: admin
        options: >-
          --health-cmd "curl -f http://localhost:80/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      webhook-receiver:
        image: tarampampam/webhook-tester:latest
        ports:
          - 8081:8080
        env:
          LISTEN_ADDR: 0.0.0.0
          LISTEN_PORT: 8080
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.22.10'
        cache: true

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.9.8
        terraform_wrapper: false

    - name: Download dependencies
      run: go mod download

    - name: Build provider binary
      run: go build .

    - name: Install provider for HCL tests
      run: |
        # Install provider to local plugin directory for Terraform HCL tests
        PROVIDER_DIR="$HOME/.terraform.d/plugins/registry.terraform.io/micah/pbs/1.0.0/linux_amd64"
        mkdir -p "$PROVIDER_DIR"
        cp terraform-provider-pbs "$PROVIDER_DIR/"
        chmod +x "$PROVIDER_DIR/terraform-provider-pbs"
        echo "Provider installed to $PROVIDER_DIR"

    - name: Get runner IP address
      id: get_ip
      run: |
        # Get the IP address of the runner on the local network
        RUNNER_IP=$(hostname -I | awk '{print $1}')
        echo "Runner IP: $RUNNER_IP"
        echo "RUNNER_IP=$RUNNER_IP" >> $GITHUB_OUTPUT

    - name: Run Terraform HCL tests
      env:
        # PBS Server configuration
        PBS_ADDRESS: "https://192.168.1.108:8007"
        PBS_USERNAME: "root@pam"
        PBS_PASSWORD: "pbspbs123"
        PBS_INSECURE_TLS: "true"
        TF_ACC: "1"
        # Test service endpoints - use runner's IP so PBS can reach them
        TEST_INFLUXDB_HOST: ${{ steps.get_ip.outputs.RUNNER_IP }}
        TEST_INFLUXDB_PORT: "8086"
        TEST_INFLUXDB_UDP_HOST: ${{ steps.get_ip.outputs.RUNNER_IP }}
        TEST_INFLUXDB_UDP_PORT: "8089"
        TEST_INFLUXDB_ORG: "testorg"
        TEST_INFLUXDB_BUCKET: "pbs-metrics"
        TEST_INFLUXDB_TOKEN: "test-token-for-pbs-provider"
        TEST_GOTIFY_HOST: ${{ steps.get_ip.outputs.RUNNER_IP }}
        TEST_GOTIFY_PORT: "8080"
        TEST_GOTIFY_TOKEN: "AqQR.Oq8ZqYJ4"
        TEST_WEBHOOK_HOST: ${{ steps.get_ip.outputs.RUNNER_IP }}
        TEST_WEBHOOK_PORT: "8081"
        # S3 provider credentials from secrets (optional for S3 tests)
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_REGION: ${{ vars.AWS_REGION || 'us-west-2' }}
        B2_ACCESS_KEY_ID: ${{ secrets.B2_ACCESS_KEY_ID }}
        B2_SECRET_ACCESS_KEY: ${{ secrets.B2_SECRET_ACCESS_KEY }}
        B2_REGION: ${{ vars.B2_REGION || 'us-west-004' }}
        SCALEWAY_ACCESS_KEY: ${{ secrets.SCALEWAY_ACCESS_KEY }}
        SCALEWAY_SECRET_KEY: ${{ secrets.SCALEWAY_SECRET_KEY }}
        SCALEWAY_REGION: ${{ vars.SCALEWAY_REGION || 'fr-par' }}
        # Set Terraform variables for HCL tests
        TF_VAR_pbs_endpoint: "https://192.168.1.108:8007"
        TF_VAR_pbs_username: "root@pam"
        TF_VAR_pbs_password: "pbspbs123"
        # Unique test run identifier to avoid name conflicts
        TF_VAR_test_id: "run-${{ github.run_number }}-${{ github.run_attempt }}"
        # Service endpoints for metrics/notification tests
        TF_VAR_influxdb_host: ${{ steps.get_ip.outputs.RUNNER_IP }}
        TF_VAR_influxdb_port: "8086"
        TF_VAR_influxdb_udp_host: ${{ steps.get_ip.outputs.RUNNER_IP }}
        TF_VAR_influxdb_udp_port: "8089"
        TF_LOG: "INFO"
      run: |
        echo ""
        echo "========================================"
        echo "Running Terraform HCL Tests"
        echo "========================================"
        echo ""
        echo "These tests use Terraform's native test framework (v1.6+)"
        echo "They replace the Go tfexec tests with native HCL execution."
        echo ""
        
        # Define all test directories
        TEST_DIRS=(
          "datastores_datasource"
          "prune_job_datasource"
          "prune_jobs_datasource"
          "sync_job_datasource"
          "datastores"
          "datastore_immutability"
          "jobs"
          "remotes"
          "metrics"
          "notifications"
          "datasources"
        )
        
        # Run tests in each directory
        for dir in "${TEST_DIRS[@]}"; do
          if [ -d "test/tftest/$dir" ]; then
            echo ""
            echo "→ Testing $dir..."
            (cd "test/tftest/$dir" && terraform init -input=false && terraform test) || {
              echo "❌ Tests failed in $dir"
              exit 1
            }
          else
            echo "⚠️  Directory not found: test/tftest/$dir (skipping)"
          fi
        done
        
        echo ""
        echo "✅ All HCL tests passed!"

    - name: Trigger PBS cleanup workflow
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          console.log('Triggering cleanup workflow in mcfitz2/lab2...');
          
          const response = await github.rest.actions.createWorkflowDispatch({
            owner: 'mcfitz2',
            repo: 'lab2',
            workflow_id: 'pbs-testing-cleanup-datastore.yml',
            ref: 'main'
          });
          
          console.log('Cleanup workflow triggered successfully');
          console.log('Response status:', response.status);
          
          // Wait a few seconds for the workflow to start
          await new Promise(resolve => setTimeout(resolve, 5000));
          
          // Get the latest run
          const runs = await github.rest.actions.listWorkflowRuns({
            owner: 'mcfitz2',
            repo: 'lab2',
            workflow_id: 'pbs-testing-cleanup-datastore.yml',
            per_page: 1
          });
          
          if (runs.data.workflow_runs.length > 0) {
            const runId = runs.data.workflow_runs[0].id;
            console.log(`Monitoring cleanup workflow run: ${runId}`);
            console.log(`URL: https://github.com/mcfitz2/lab2/actions/runs/${runId}`);
            
            // Poll for completion
            let completed = false;
            let attempts = 0;
            const maxAttempts = 60; // 5 minutes (5 seconds per attempt)
            
            while (!completed && attempts < maxAttempts) {
              await new Promise(resolve => setTimeout(resolve, 5000));
              attempts++;
              
              const run = await github.rest.actions.getWorkflowRun({
                owner: 'mcfitz2',
                repo: 'lab2',
                run_id: runId
              });
              
              console.log(`Attempt ${attempts}: Status = ${run.data.status}, Conclusion = ${run.data.conclusion}`);
              
              if (run.data.status === 'completed') {
                completed = true;
                if (run.data.conclusion === 'success') {
                  console.log('✅ Cleanup workflow completed successfully');
                } else {
                  console.log(`⚠️  Cleanup workflow completed with conclusion: ${run.data.conclusion}`);
                }
              }
            }
            
            if (!completed) {
              console.log('⚠️  Cleanup workflow did not complete within timeout period');
            }
          } else {
            console.log('⚠️  Could not find the triggered workflow run');
          }

    - name: Upload coverage
      uses: actions/upload-artifact@v4
      with:
        name: coverage-vm-integration
        path: coverage-vm-integration.out
        retention-days: 7

    - name: Report coverage
      uses: codecov/codecov-action@v4
      with:
        files: ./coverage-vm-integration.out
        flags: vm-integration-tests
        name: vm-integration-coverage

    - name: Upload test logs
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: vm-test-logs-pbs-4.0
        path: |
          test/integration/*.log
          terraform.log
        retention-days: 7